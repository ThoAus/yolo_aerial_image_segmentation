{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d4634de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.onnx\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e26e6d9",
   "metadata": {},
   "source": [
    "# 01 - Load model and make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2012bec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO11m-seg summary: 253 layers, 22,360,758 parameters, 0 gradients, 123.6 GFLOPs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(253, 22360758, 0, 123.58435839999999)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model\n",
    "path_to_model = 'C:/Users/thoma/Documents/GitHub/WING_YOLO/runs/segment/RUN_10/weights/best.pt'\n",
    "model = YOLO(path_to_model)\n",
    "model.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a915e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all test images in the directory\n",
    "test_images = list(Path('test_images').glob('*.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2518a5",
   "metadata": {},
   "source": [
    "https://docs.ultralytics.com/modes/predict/#inference-arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b66a1f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 38 Roofs, 438.8ms\n",
      "1: 640x640 2 Roofs, 438.8ms\n",
      "2: 640x640 6 Roofs, 438.8ms\n",
      "Speed: 3.5ms preprocess, 438.8ms inference, 15.6ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on a test image with YOLO\n",
    "results = model(\n",
    "    source=test_images,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4124d5",
   "metadata": {},
   "source": [
    "https://docs.ultralytics.com/reference/engine/results/#ultralytics.engine.results.Results.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ccd9e53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the results\n",
    "for pred in results:\n",
    "    pred.show(\n",
    "        boxes=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fece9e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a185326e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad6a1c68",
   "metadata": {},
   "source": [
    "# 02 - Extract segmentation masks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7327ef",
   "metadata": {},
   "source": [
    "Export ONXX for Deepness  \n",
    "https://qgis-plugin-deepness.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78aa538c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de519c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8a20ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume 'results' is the output from model(), e.g. results = model(img)\n",
    "masks = results[1].masks  # ultralytics.engine.results.Masks object\n",
    "classes = results[1].boxes.cls.int()  # Tensor of class indices for each mask\n",
    "num_classes = int(classes.max().item()) + 1  # Or set manually if known\n",
    "\n",
    "# masks.data: (num_instances, H, W)\n",
    "mask_data = masks.data  # shape: (num_instances, H, W)\n",
    "H, W = mask_data.shape[1:]\n",
    "\n",
    "# Create one-hot tensor: (num_classes, H, W)\n",
    "onehot = torch.zeros((num_classes, H, W), dtype=torch.uint8)\n",
    "\n",
    "for idx, class_idx in enumerate(classes):\n",
    "    onehot[class_idx] |= (mask_data[idx] > 0).byte()\n",
    "\n",
    "# 'onehot' now has a channel for each class, with 1s where that class is present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e4be97",
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309d4a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the one-hot encoded masks\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(num_classes):\n",
    "    plt.subplot(1, num_classes, i + 1)\n",
    "    plt.imshow(onehot[i].cpu().numpy(), cmap='gray')\n",
    "    plt.title(f'Class {i}')\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf5da0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb164cff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cace6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0].masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd4346d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7aa98a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c7d7e0e",
   "metadata": {},
   "source": [
    "## Extract model to PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d9d243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image\n",
    "image = Image.open('test_image.png').convert('RGB')\n",
    "\n",
    "# Transorm image to tensor\n",
    "image_to_tensor = transforms.ToTensor()\n",
    "image_tensor = image_to_tensor(image).unsqueeze(0)  # add batch dimension\n",
    "\n",
    "print(f'Shape: {image_tensor.shape}')\n",
    "print(f'Values (min/max): {image_tensor.min(), image_tensor.max()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6313ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the pytorch model\n",
    "pytorch_model = model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd474f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions using the pytorch model\n",
    "pred = pytorch_model(image_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5913383f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d64d11e",
   "metadata": {},
   "source": [
    "### Explore model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33127352",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(pred), len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04236ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the object in tuple\n",
    "for i in pred:\n",
    "    print(type(i), len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814d70e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First object in tuple\n",
    "pred[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e04b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the objects in the tuple\n",
    "for i in pred[1]:\n",
    "    print(type(i), len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c195d04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the content in the list\n",
    "for i in pred[1][0]:\n",
    "    print(type(i), len(i), i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d580696",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[1][1].shape, pred[1][2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59bc0bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc35f972",
   "metadata": {},
   "source": [
    "## Export model to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6706d63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "path_to_model = 'C:/Users/thoma/Documents/GitHub/WING_YOLO/runs/segment/RUN_10/weights/best.pt'\n",
    "model = YOLO(path_to_model)\n",
    "\n",
    "# Export the model to ONNX format\n",
    "model.export(\n",
    "    format='onnx',\n",
    "    dynamic=True,\n",
    "    simplify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b449de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import onnxruntime\n",
    "\n",
    "# Load the ONNX model\n",
    "onnx_path = 'C:/Users/thoma/Documents/GitHub/WING_YOLO/runs/segment/RUN_10/weights/best.onnx'\n",
    "onnx_model = onnx.load(onnx_path)\n",
    "\n",
    "# Check the model\n",
    "onnx.checker.check_model(onnx_model)\n",
    "\n",
    "# Create an inference session using ONNX Runtime\n",
    "ort_session = onnxruntime.InferenceSession(onnx_path)\n",
    "\n",
    "def to_numpy(tensor):\n",
    "\treturn tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "# Make predictions with the ONNX model\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(image_tensor)}\n",
    "ort_outs = ort_session.run(None, ort_inputs)\n",
    "\n",
    "# Print the results\n",
    "type(ort_outs), len(ort_outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f887231",
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_outs[0].shape, ort_outs[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f58717e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a7fccd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5d09df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2bc9f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906c1238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on a random tensor\n",
    "x = torch.rand(1, 3, 640, 640)\n",
    "_ = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf5ecca",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(\n",
    "    pytorch_model,\n",
    "                x,  # model input\n",
    "                'model.onnx',  # where to save the model\n",
    "                export_params=True,\n",
    "                opset_version=15,\n",
    "                input_names=['input'],\n",
    "                output_names=['output'],\n",
    "                do_constant_folding=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8bf781",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
